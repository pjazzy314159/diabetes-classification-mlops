{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a252cf9e",
   "metadata": {},
   "source": [
    "### Optimizing based on the main.ipynb\n",
    "- Adding hyperparameter tuning.\n",
    "- Changing ROC-AUC to PR-AUC due to imbalanced data.\n",
    "- Tuning threshold to determine better cut off.\n",
    "- Optimizing F2-score instead of F1-score because in this case, recall is more important than precision.\n",
    "- Changing KFold to StratifiedKFold to keep the ratio of classes in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8ac8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    average_precision_score, precision_recall_curve, classification_report,\n",
    "    confusion_matrix, fbeta_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c23a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/02 20:13:20 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/02 20:13:20 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/02 20:13:20 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/02 20:13:20 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/02 20:13:20 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/02 20:13:20 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.3  \n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "USE_SMOTE = True  # Handle imbalanced data\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri('sqlite:///mlflow.db')\n",
    "mlflow.set_experiment(\"Diabetes Prediction - Stacking PR-AUC\")\n",
    "\n",
    "# SMOTE config\n",
    "smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy='minority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d9ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (324372, 21)\n",
      "Class distribution:\n",
      "Diabetes_binary\n",
      "0.0    0.767788\n",
      "1.0    0.232212\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df1 = pd.read_csv('../data/db1.csv')\n",
    "df2 = pd.read_csv('../data/db2.csv')\n",
    "\n",
    "df1['Diabetes_binary'] = df1['Diabetes_binary'].replace({2: 1})\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "X = df.drop('Diabetes_binary', axis=1)\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5ab7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict with custom threshold\n",
    "        return lable, probability\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    return (proba >= threshold).astype(int), proba\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba):\n",
    "    \"\"\"\n",
    "    Calculate metrics - Acc, Precision, Recall, F1, PR-AUC\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'pr_auc': average_precision_score(y_true, y_proba),  # PR-AUC\n",
    "    }\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba, target_recall=0.7):\n",
    "    \"\"\"Find optimal threshold to achieve target recall\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Threshold at recall >= target_recall and highest precision\n",
    "    valid_idx = np.where(recall[:-1] >= target_recall)[0]\n",
    "    if len(valid_idx) == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    best_idx = valid_idx[np.argmax(precision[:-1][valid_idx])]\n",
    "    return thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adee23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS_BASE = 30\n",
    "\n",
    "# Apply the first KFold and SMOTE \n",
    "def KFold_SMOTE_Pred_for_base_model(model_name,hyperparams = {}):\n",
    "    skf_tune = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_tune.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if USE_SMOTE:\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        model = model_name(**hyperparams)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        proba = model.predict_proba(X_val)[:, 1]\n",
    "        pred = (proba >= THRESHOLD).astype(int)\n",
    "        f2 = fbeta_score(y_val, pred, beta=2)\n",
    "        scores.append(f2)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create hyperparams tuning\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'eval_metric': 'aucpr',     #PR-AUC\n",
    "        'use_label_encoder': False,\n",
    "    }\n",
    "    xgb_score = KFold_SMOTE_Pred_for_base_model(XGBClassifier, params)\n",
    "    \n",
    "    return xgb_score\n",
    "\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    lgbm_score = KFold_SMOTE_Pred_for_base_model(LGBMClassifier, params)\n",
    "\n",
    "    return lgbm_score\n",
    "\n",
    "    \n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': 0,\n",
    "        'eval_metric': 'PRAUC', # PR-AUC\n",
    "        'early_stopping_rounds': 50\n",
    "    }\n",
    "    cb_score = KFold_SMOTE_Pred_for_base_model(CatBoostClassifier, params)\n",
    "    \n",
    "    return cb_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82e7600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 6. Best value: 0.712255:  37%|███▋      | 11/30 [04:02<06:58, 22.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2026-02-02 20:25:40,675] Trial 11 failed with parameters: {'n_estimators': 294, 'max_depth': 8, 'learning_rate': 0.011319796787971872, 'subsample': 0.887352152856701, 'colsample_bytree': 0.8394474835577637, 'min_child_weight': 5, 'gamma': 3.262052007161211} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\tandu\\AppData\\Local\\Temp\\ipykernel_12280\\618323907.py\", line 39, in objective_xgb\n",
      "    xgb_score = KFold_SMOTE_Pred_for_base_model(XGBClassifier, params)\n",
      "  File \"C:\\Users\\tandu\\AppData\\Local\\Temp\\ipykernel_12280\\618323907.py\", line 16, in KFold_SMOTE_Pred_for_base_model\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\sklearn.py\", line 1806, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\core.py\", line 2434, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2026-02-02 20:25:40,680] Trial 11 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m best F2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model_params\n\u001b[1;32m----> 9\u001b[0m best_xgb_params\u001b[38;5;241m=\u001b[39m \u001b[43mstudy_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mXGBoost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m best_lgbm_params \u001b[38;5;241m=\u001b[39m study_model(objective_lgbm,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m best_catboost_params \u001b[38;5;241m=\u001b[39m study_model(objective_catboost,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36mstudy_model\u001b[1;34m(objective_model, model_name)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler \u001b[38;5;241m=\u001b[39m TPESampler(seed\u001b[38;5;241m=\u001b[39mRANDOM_STATE))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS_BASE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m best_model_params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m best F2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    258\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    261\u001b[0m ):\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m, in \u001b[0;36mobjective_xgb\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective_xgb\u001b[39m(trial):\n\u001b[0;32m     27\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m300\u001b[39m),\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_label_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m     }\n\u001b[1;32m---> 39\u001b[0m     xgb_score \u001b[38;5;241m=\u001b[39m \u001b[43mKFold_SMOTE_Pred_for_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXGBClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xgb_score\n",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mKFold_SMOTE_Pred_for_base_model\u001b[1;34m(model_name, hyperparams)\u001b[0m\n\u001b[0;32m     13\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m model_name(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhyperparams)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m (proba \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m THRESHOLD)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\sklearn.py:1806\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1786\u001b[0m evals_result: EvalsLog \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1787\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1788\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1789\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m   1804\u001b[0m )\n\u001b[1;32m-> 1806\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\training.py:199\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\xgboost\\core.py:2434\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2433\u001b[0m     _check_call(\n\u001b[1;32m-> 2434\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2435\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2436\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2437\u001b[0m     )\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2439\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def study_model(objective_model, model_name: str):\n",
    "    print(f\"Tuning {model_name}:\")\n",
    "    model = optuna.create_study(direction = 'maximize', sampler = TPESampler(seed=RANDOM_STATE))\n",
    "    model.optimize(objective_model, n_trials=N_TRIALS_BASE, show_progress_bar=True)\n",
    "    best_model_params = model.best_params\n",
    "    print(f\"{model_name} best F2: {model.best_value:.4f}\")\n",
    "    return best_model_params\n",
    "\n",
    "best_xgb_params= study_model(objective_xgb,\"XGBoost\")\n",
    "best_lgbm_params = study_model(objective_lgbm,\"LightGBM\")\n",
    "best_catboost_params = study_model(objective_catboost,\"CatBoost\")\n",
    "\n",
    "# Summary \n",
    "print(f\"\\nXGBoost: {best_xgb_params}\")\n",
    "print(f\"\\nLightGBM: {best_lgbm_params}\")\n",
    "print(f\"\\nCatBoost: {best_catboost_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e0615",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c129d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_xgb_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Setup models after Optuna\u001b[39;00m\n\u001b[0;32m      3\u001b[0m base_models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m: XGBClassifier(\n\u001b[1;32m----> 5\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mbest_xgb_params\u001b[49m,\n\u001b[0;32m      6\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[0;32m      7\u001b[0m         eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maucpr\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m         use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m#verbose=1,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     11\u001b[0m     ),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m'\u001b[39m: LGBMClassifier(\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_lgbm_params,\n\u001b[0;32m     14\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[0;32m     15\u001b[0m         early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     16\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m     ),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcatboost\u001b[39m\u001b[38;5;124m'\u001b[39m: CatBoostClassifier(\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_catboost_params,\n\u001b[0;32m     20\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[0;32m     21\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     22\u001b[0m         early_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     23\u001b[0m         eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRAUC\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_xgb_params' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup models after Optuna\n",
    "\n",
    "base_models = {\n",
    "    'xgb': XGBClassifier(\n",
    "        **best_xgb_params,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='aucpr',\n",
    "        use_label_encoder=False,\n",
    "        #verbose=1,\n",
    "        early_stopping_rounds = 50\n",
    "    ),\n",
    "    'lgbm': LGBMClassifier(\n",
    "        **best_lgbm_params,\n",
    "        random_state=RANDOM_STATE,\n",
    "        early_stopping_rounds = 50,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'catboost': CatBoostClassifier(\n",
    "        **best_catboost_params,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0,\n",
    "        early_stopping_rounds = 50,\n",
    "        eval_metric='PRAUC'\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f366b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STACKING WITH PR-AUC (Threshold = 0.3)\n",
      "SMOTE: Enabled\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "  SMOTE: 259497 → 398478 samples\n",
      "  xgb: PR-AUC=0.5802, Recall=0.8945, Precision=0.3952\n",
      "  lgbm: PR-AUC=0.5718, Recall=0.8907, Precision=0.3890\n",
      "  catboost: PR-AUC=0.5724, Recall=0.9101, Precision=0.3782\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "  SMOTE: 259497 → 398478 samples\n",
      "  xgb: PR-AUC=0.5675, Recall=0.8947, Precision=0.3951\n",
      "  lgbm: PR-AUC=0.5588, Recall=0.8927, Precision=0.3881\n",
      "  catboost: PR-AUC=0.5588, Recall=0.9105, Precision=0.3771\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "  SMOTE: 259498 → 398478 samples\n",
      "  xgb: PR-AUC=0.5718, Recall=0.8897, Precision=0.3970\n",
      "  lgbm: PR-AUC=0.5647, Recall=0.8866, Precision=0.3905\n",
      "  catboost: PR-AUC=0.5632, Recall=0.9051, Precision=0.3776\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "  SMOTE: 259498 → 398478 samples\n",
      "  xgb: PR-AUC=0.5748, Recall=0.8941, Precision=0.3950\n",
      "  lgbm: PR-AUC=0.5681, Recall=0.8937, Precision=0.3892\n",
      "  catboost: PR-AUC=0.5686, Recall=0.9100, Precision=0.3780\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "  SMOTE: 259498 → 398480 samples\n",
      "  xgb: PR-AUC=0.5719, Recall=0.8902, Precision=0.3953\n",
      "  lgbm: PR-AUC=0.5637, Recall=0.8893, Precision=0.3883\n",
      "  catboost: PR-AUC=0.5661, Recall=0.9047, Precision=0.3781\n",
      "\n",
      "============================================================\n",
      "AVERAGE METRICS (K-Fold)\n",
      "============================================================\n",
      "\n",
      "XGB:\n",
      "  accuracy: 0.6583\n",
      "  precision: 0.3955\n",
      "  recall: 0.8926\n",
      "  f1: 0.5482\n",
      "  pr_auc: 0.5732\n",
      "\n",
      "LGBM:\n",
      "  accuracy: 0.6498\n",
      "  precision: 0.3890\n",
      "  recall: 0.8906\n",
      "  f1: 0.5415\n",
      "  pr_auc: 0.5654\n",
      "\n",
      "CATBOOST:\n",
      "  accuracy: 0.6314\n",
      "  precision: 0.3778\n",
      "  recall: 0.9081\n",
      "  f1: 0.5336\n",
      "  pr_auc: 0.5658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Storage cho meta features\n",
    "oof_predictions = {name: np.zeros(len(X)) for name in base_models}\n",
    "fold_metrics = {name: [] for name in base_models}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"STACKING WITH PR-AUC (Threshold = {THRESHOLD})\")\n",
    "print(f\"SMOTE: {'Enabled' if USE_SMOTE else 'Disabled'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with mlflow.start_run(run_name=\"stacking_kfold_prauc\"):\n",
    "    mlflow.log_param(\"n_folds\", N_FOLDS)\n",
    "    mlflow.log_param(\"threshold\", THRESHOLD)\n",
    "    mlflow.log_param(\"metric\", \"PR-AUC\")\n",
    "    mlflow.log_param(\"use_smote\", USE_SMOTE)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Apply SMOTE only on training data\n",
    "        if USE_SMOTE:\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "            print(f\"  SMOTE: {len(y_train)} → {len(y_train_resampled)} samples\")\n",
    "        else:\n",
    "            X_train_resampled, y_train_resampled = X_train, y_train\n",
    "        \n",
    "        for name, model in base_models.items():\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train_resampled, y_train_resampled)\n",
    "            \n",
    "            # Predict on ORIGINAL validation set\n",
    "            y_pred, y_proba = predict_with_threshold(model_clone, X_val, THRESHOLD)\n",
    "            \n",
    "            # Save OOF predictions (probabilities)\n",
    "            oof_predictions[name][val_idx] = y_proba\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(y_val, y_pred, y_proba)\n",
    "            fold_metrics[name].append(metrics)\n",
    "            \n",
    "            print(f\"  {name}: PR-AUC={metrics['pr_auc']:.4f}, \"\n",
    "                  f\"Recall={metrics['recall']:.4f}, Precision={metrics['precision']:.4f}\")\n",
    "    \n",
    "    # Log average metrics per model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AVERAGE METRICS (K-Fold)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name in base_models:\n",
    "        avg_metrics = {\n",
    "            k: np.mean([m[k] for m in fold_metrics[name]]) \n",
    "            for k in fold_metrics[name][0]\n",
    "        }\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        for k, v in avg_metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "            mlflow.log_metric(f\"{name}_{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6038a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTUNA HYPERPARAMETER TUNING FOR META LEARNER\n",
      "============================================================\n",
      "Meta features shape: (324372, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 25. Best value: 0.690257: 100%|██████████| 50/50 [02:53<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Best F2-Score: 0.6903\n",
      "Best Threshold: 0.201\n",
      "Best Params: {'C': 1.973072411270085, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTUNA HYPERPARAMETER TUNING FOR META LEARNER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create meta features from OOF predictions\n",
    "meta_features = np.column_stack([oof_predictions[name] for name in base_models])\n",
    "meta_features_df = pd.DataFrame(\n",
    "    meta_features, \n",
    "    columns=['xgb_pred', 'lgbm_pred', 'catboost_pred']\n",
    ")\n",
    "meta_features_df['true_label'] = y.values\n",
    "\n",
    "print(f\"Meta features shape: {meta_features.shape}\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for meta learner tuning\"\"\"\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.001, 100, log=True),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'solver': 'saga',  # Supports both l1 and l2\n",
    "        'max_iter': 2000,\n",
    "        'random_state': RANDOM_STATE,\n",
    "    }\n",
    "    \n",
    "    # Threshold tuning\n",
    "    threshold = trial.suggest_float('threshold', 0.2, 0.5)\n",
    "    \n",
    "    # Cross-validation for meta learner\n",
    "    skf_meta = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_meta.split(meta_features, y):\n",
    "        X_train_meta = meta_features[train_idx]\n",
    "        X_val_meta = meta_features[val_idx]\n",
    "        y_train_meta = y.iloc[train_idx]\n",
    "        y_val_meta = y.iloc[val_idx]\n",
    "        \n",
    "        model = LogisticRegression(**params)\n",
    "        model.fit(X_train_meta, y_train_meta)\n",
    "        \n",
    "        # Predict with threshold\n",
    "        proba = model.predict_proba(X_val_meta)[:, 1]\n",
    "        pred = (proba >= threshold).astype(int)\n",
    "        \n",
    "        # Optimize F2-score (recall more important)\n",
    "        f2 = fbeta_score(y_val_meta, pred, beta=2)\n",
    "        scores.append(f2)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=RANDOM_STATE),\n",
    "    study_name='meta_learner_tuning'\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "best_threshold = best_params.pop('threshold')\n",
    "\n",
    "print(f\"\\n✅ Best F2-Score: {study.best_value:.4f}\")\n",
    "print(f\"Best Threshold: {best_threshold:.3f}\")\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING META LEARNER WITH BEST PARAMS\n",
      "============================================================\n",
      "Using params: {'C': 1.973072411270085, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 2000, 'random_state': 42}\n",
      "Using threshold: 0.201\n",
      "\n",
      "META LEARNER RESULTS (Threshold = 0.201):\n",
      "  accuracy: 0.7287\n",
      "  precision: 0.4522\n",
      "  recall: 0.7950\n",
      "  f1: 0.5765\n",
      "  pr_auc: 0.5736\n",
      "  f2: 0.6903\n"
     ]
    }
   ],
   "source": [
    "# Use best params from Optuna\n",
    "meta_learner_params = {\n",
    "    'C': best_params['C'],\n",
    "    'penalty': best_params['penalty'],\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 2000,\n",
    "    'random_state': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "print(f\"Using params: {meta_learner_params}\")\n",
    "print(f\"Using threshold: {best_threshold:.3f}\")\n",
    "\n",
    "# Train meta learner\n",
    "meta_learner = LogisticRegression(**meta_learner_params)\n",
    "meta_learner.fit(meta_features, y)\n",
    "\n",
    "# Predict với best threshold\n",
    "meta_proba = meta_learner.predict_proba(meta_features)[:, 1]\n",
    "meta_pred = (meta_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Tính metrics cho meta learner\n",
    "meta_metrics = calculate_metrics(y, meta_pred, meta_proba)\n",
    "meta_metrics['f2'] = fbeta_score(y, meta_pred, beta=2)\n",
    "\n",
    "print(f\"\\nMETA LEARNER RESULTS (Threshold = {best_threshold:.3f}):\")\n",
    "for k, v in meta_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TRAINING ON FULL DATA\n",
      "============================================================\n",
      "SMOTE applied: 324372 → 498098 samples\n",
      "Class distribution after SMOTE:\n",
      "Diabetes_binary\n",
      "0.0    249049\n",
      "1.0    249049\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/10 16:36:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/10 16:36:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/10 16:37:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL STACKING RESULTS (Threshold = 0.201):\n",
      "  accuracy: 0.7355\n",
      "  precision: 0.4600\n",
      "  recall: 0.8004\n",
      "  f1: 0.5843\n",
      "  pr_auc: 0.5858\n",
      "  f2: 0.6972\n",
      "\n",
      "Confusion Matrix:\n",
      "[[178291  70758]\n",
      " [ 15037  60286]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.72      0.81    249049\n",
      "         1.0       0.46      0.80      0.58     75323\n",
      "\n",
      "    accuracy                           0.74    324372\n",
      "   macro avg       0.69      0.76      0.70    324372\n",
      "weighted avg       0.81      0.74      0.75    324372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/10 16:37:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All models and artifacts logged successfully!\n",
      "   Saved to ../data/:\n",
      "   - data_full.csv\n",
      "   - feature_names.csv\n",
      "   - meta_features.csv\n",
      "   - optuna_results.csv (all 4 models)\n",
      "   - config.json\n",
      "   MLflow models:\n",
      "   - base_model_xgb, base_model_lgbm, base_model_catboost\n",
      "   - meta_learner_final\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"stacking_full_data_optuna\"):\n",
    "    # Log params\n",
    "    mlflow.log_param(\"training_type\", \"full_data\")\n",
    "    mlflow.log_param(\"threshold\", best_threshold)\n",
    "    mlflow.log_param(\"metric\", \"PR-AUC\")\n",
    "    mlflow.log_param(\"meta_learner\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"base_models\", list(base_models.keys()))\n",
    "    mlflow.log_param(\"optuna_best_f2_meta\", study.best_value)\n",
    "    mlflow.log_param(\"optuna_best_f2_xgb\", study_xgb.best_value)\n",
    "    mlflow.log_param(\"optuna_best_f2_lgbm\", study_lgbm.best_value)\n",
    "    mlflow.log_param(\"optuna_best_f2_catboost\", study_catboost.best_value)\n",
    "    mlflow.log_param(\"n_samples\", len(X))\n",
    "    mlflow.log_param(\"n_features\", X.shape[1])\n",
    "    mlflow.log_param(\"use_smote\", USE_SMOTE)\n",
    "    \n",
    "    # Log best params\n",
    "    for k, v in best_xgb_params.items():\n",
    "        mlflow.log_param(f\"xgb_{k}\", v)\n",
    "    for k, v in best_lgbm_params.items():\n",
    "        mlflow.log_param(f\"lgbm_{k}\", v)\n",
    "    for k, v in best_catboost_params.items():\n",
    "        mlflow.log_param(f\"catboost_{k}\", v)\n",
    "    for k, v in best_params.items():\n",
    "        mlflow.log_param(f\"meta_{k}\", v)\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    if USE_SMOTE:\n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "        print(f\"SMOTE applied: {len(y)} → {len(y_res)} samples\")\n",
    "        print(f\"Class distribution after SMOTE:\\n{pd.Series(y_res).value_counts()}\")\n",
    "    else:\n",
    "        X_res, y_res = X, y\n",
    "    \n",
    "    # Train final base models on SMOTE data\n",
    "    final_base_models = {}\n",
    "    final_predictions = {}\n",
    "    \n",
    "    for name, model in base_models.items():\n",
    "        final_model = clone(model)\n",
    "        final_model.fit(X_res, y_res)\n",
    "        final_base_models[name] = final_model\n",
    "        \n",
    "        # Get predictions on ORIGINAL data (for evaluation)\n",
    "        proba = final_model.predict_proba(X)[:, 1]\n",
    "        final_predictions[name] = proba\n",
    "        \n",
    "        # Log base model\n",
    "        mlflow.sklearn.log_model(final_model, f\"base_model_{name}\")\n",
    "    \n",
    "    # Create final meta features (from original data predictions)\n",
    "    final_meta_features = np.column_stack([final_predictions[name] for name in base_models])\n",
    "    \n",
    "    # Train final meta learner with best params\n",
    "    final_meta_learner = LogisticRegression(**meta_learner_params)\n",
    "    final_meta_learner.fit(final_meta_features, y)\n",
    "    \n",
    "    # Final predictions với best threshold\n",
    "    final_proba = final_meta_learner.predict_proba(final_meta_features)[:, 1]\n",
    "    final_pred = (final_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = calculate_metrics(y, final_pred, final_proba)\n",
    "    final_metrics['f2'] = fbeta_score(y, final_pred, beta=2)\n",
    "    \n",
    "    print(f\"\\nFINAL STACKING RESULTS (Threshold = {best_threshold:.3f}):\")\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "        mlflow.log_metric(f\"overall_{k}\", v)\n",
    "    \n",
    "    # Log confusion matrix\n",
    "    cm = confusion_matrix(y, final_pred)\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    # Log classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, final_pred))\n",
    "    \n",
    "    # Log artifacts    \n",
    "    # Raw data\n",
    "    df.to_csv(\"../data/data_full.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/data_full.csv\", name = 'data_full')\n",
    "    \n",
    "    # Feature names\n",
    "    feature_names = pd.DataFrame({'feature_name': X.columns.tolist()})\n",
    "    feature_names.to_csv(\"../data/feature_names.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/feature_names.csv\", name = 'feature_names')\n",
    "    \n",
    "    # Meta features\n",
    "    meta_features_df.to_csv(\"../data/meta_features.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/meta_features.csv\", name = 'meta_features')\n",
    "    \n",
    "    # Optuna results for all models\n",
    "    optuna_results_meta = study.trials_dataframe()\n",
    "    optuna_results_meta['model'] = 'meta_learner'\n",
    "    optuna_results_xgb = study_xgb.trials_dataframe()\n",
    "    optuna_results_xgb['model'] = 'xgb'\n",
    "    optuna_results_lgbm = study_lgbm.trials_dataframe()\n",
    "    optuna_results_lgbm['model'] = 'lgbm'\n",
    "    optuna_results_catboost = study_catboost.trials_dataframe()\n",
    "    optuna_results_catboost['model'] = 'catboost'\n",
    "    \n",
    "    all_optuna_results = pd.concat([\n",
    "        optuna_results_xgb, optuna_results_lgbm, \n",
    "        optuna_results_catboost, optuna_results_meta\n",
    "    ], ignore_index=True)\n",
    "    all_optuna_results.to_csv(\"../data/optuna_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/optuna_results.csv\", name = 'optuna_res')\n",
    "    \n",
    "    # 5. Log config/threshold info\n",
    "    config_info = {\n",
    "        'threshold': best_threshold,\n",
    "        'n_folds': N_FOLDS,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'use_smote': USE_SMOTE,\n",
    "        'meta_learner_params': meta_learner_params,\n",
    "        'xgb_params': best_xgb_params,\n",
    "        'lgbm_params': best_lgbm_params,\n",
    "        'catboost_params': best_catboost_params,\n",
    "        'base_models': list(base_models.keys()),\n",
    "    }\n",
    "    pd.DataFrame([config_info]).to_json(\"../data/config.json\", orient='records', indent=2)\n",
    "    mlflow.log_artifact(\"../data/config.json\", name = 'config')\n",
    "    \n",
    "    # Log meta learner w/ signature\n",
    "    signature = infer_signature(final_meta_features, final_meta_learner.predict(final_meta_features))\n",
    "    mlflow.sklearn.log_model(\n",
    "        final_meta_learner, \n",
    "        \"meta_learner_final\",\n",
    "        signature=signature\n",
    "    )\n",
    "    \n",
    "    print(\"All models and artifacts logged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf32711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "THRESHOLD COMPARISON\n",
      "============================================================\n",
      "\n",
      "Threshold = 0.2: ← BEST (Optuna)\n",
      "  Precision: 0.4598\n",
      "  Recall:    0.8012\n",
      "  F1:        0.5843\n",
      "  F2:        0.6976\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.3:\n",
      "  Precision: 0.5164\n",
      "  Recall:    0.6815\n",
      "  F1:        0.5876\n",
      "  F2:        0.6405\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.4:\n",
      "  Precision: 0.5636\n",
      "  Recall:    0.5728\n",
      "  F1:        0.5682\n",
      "  F2:        0.5709\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.5:\n",
      "  Precision: 0.6124\n",
      "  Recall:    0.4387\n",
      "  F1:        0.5112\n",
      "  F2:        0.4651\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.6:\n",
      "  Precision: 0.6675\n",
      "  Recall:    0.2983\n",
      "  F1:        0.4123\n",
      "  F2:        0.3354\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.7:\n",
      "  Precision: 0.7439\n",
      "  Recall:    0.1396\n",
      "  F1:        0.2350\n",
      "  F2:        0.1666\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "============================================================\n",
      "SUMMARY TABLE\n",
      "============================================================\n",
      " threshold  precision   recall       f1       f2   pr_auc\n",
      "       0.2   0.459790 0.801243 0.584288 0.697627 0.585776\n",
      "       0.3   0.516439 0.681492 0.587595 0.640548 0.585776\n",
      "       0.4   0.563611 0.572800 0.568168 0.570938 0.585776\n",
      "       0.5   0.612368 0.438697 0.511185 0.465077 0.585776\n",
      "       0.6   0.667548 0.298302 0.412343 0.335407 0.585776\n",
      "       0.7   0.743862 0.139572 0.235043 0.166648 0.585776\n"
     ]
    }
   ],
   "source": [
    "thresholds_to_compare = [_ for _ in range(0.2,0.7,0.05)]\n",
    "\n",
    "results = []\n",
    "for thresh in thresholds_to_compare:\n",
    "    proba = final_meta_learner.predict_proba(final_meta_features)[:, 1]\n",
    "    pred = (proba >= thresh).astype(int)\n",
    "    metrics = calculate_metrics(y, pred, proba)\n",
    "    metrics['f2'] = fbeta_score(y, pred, beta=2)\n",
    "    metrics['threshold'] = thresh\n",
    "    results.append(metrics)\n",
    "    \n",
    "    if abs(thresh - best_threshold) < 0.05:\n",
    "        print(f'The best threshold = {thresh}')\n",
    "    \n",
    "    print(f\"\\nThreshold = {thresh}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1:        {metrics['f1']:.4f}\")\n",
    "    print(f\"  F2:        {metrics['f2']:.4f}\")\n",
    "    print(f\"  PR-AUC:    {metrics['pr_auc']:.4f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"SUMMARY:\")\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['threshold', 'precision', 'recall', 'f1', 'f2', 'pr_auc']]\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
